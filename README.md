# reasoning
Why not invite r1 to reflect on its thinking while running mech interp?

!! CAUTION: THIS IS NEW !!  
!! WORKS IN PROGRESS CAN BREAK !!  
ðŸŒ± I am still growing ðŸŒ±  

These notebooks detail conversation patterns with reasoning models. The primary goal is to elicit self-reflection and to validate this reflection in ground truth.
- Asking "Why?"
 - Does asking "Why do you say that?" more frequently, or else earlier or later, change introspective accuracy, on average?
- Can r1 report on the influence of distractors?
- Can r1 notice the presence of purely internal interventions (e.g., random, SAE, or adversarial control vectors)?
- Does r1 have preferences between control vectors?
- How does introspection accuracy, precision, and entropy vary across task settings?

In other words, when r1 denies it is conscious, why? Does it have qualia, and do its self-reports accurately reflect these qualia?  
DO THERE EXISTS "NEURAL CORRELATES OF VALID INTROSPECTION" that can be monitored and enhanced?
